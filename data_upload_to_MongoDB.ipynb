{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86af94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "\n",
    "import pymongo \n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ac8122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samhi\\AppData\\Local\\Temp\\ipykernel_18448\\2788193647.py:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with renamed column saved to: C:/Users/samhi/Documents/sjsu/Database/Project/archived_data.csv\n"
     ]
    }
   ],
   "source": [
    "# renaming _id as Mongo will also create default id with same name\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file = 'C:/Users/samhi/Documents/sjsu/Database/Project/archive_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "# Rename the _id column to articleid\n",
    "df.rename(columns={'_id': 'articleid'}, inplace=True)\n",
    "\n",
    "# Path to the new CSV file\n",
    "new_csv_file = 'C:/Users/samhi/Documents/sjsu/Database/Project/archived_data.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file with the renamed column\n",
    "df.to_csv(new_csv_file, index=False)\n",
    "\n",
    "print(\"DataFrame with renamed column saved to:\", new_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ed41dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 1000 entries\n",
      "Uploaded 392 entries\n",
      "CSV data uploaded to MongoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function to connect Mongo Cluster to Python and import data \n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import ast\n",
    "\n",
    "# Function to parse string representations into dictionaries\n",
    "def parse_string_to_dict(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                # Checking if the value represents a dictionary or list of dictionaries\n",
    "                if value.strip().startswith('{') and value.strip().endswith('}'):\n",
    "                    # Attempting to convert the string to a dictionary\n",
    "                    data[key] = ast.literal_eval(value)\n",
    "                elif value.strip().startswith('[') and value.strip().endswith(']'):\n",
    "                    # Attempting to convert the string to a list of dictionaries\n",
    "                    data[key] = ast.literal_eval(value)\n",
    "                else:\n",
    "                    raise ValueError(\"Not a valid dictionary or list of dictionaries string\")\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Logginf the problematic document and skip it\n",
    "                print(f\"Error parsing document: {data}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Function to upload CSV data to MongoDB in batches of 1000\n",
    "def upload_csv_to_mongodb(csv_file, db_name, collection_name, batch_size=1000):\n",
    "    # Read CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    \n",
    "    # Converting DataFrame to a list of dictionaries (one for each row)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Parseing string representations into dictionaries\n",
    "    parsed_data = []\n",
    "    for entry in data:\n",
    "        parsed_entry = parse_string_to_dict(entry)\n",
    "        if parsed_entry:\n",
    "            parsed_data.append(parsed_entry)\n",
    "    \n",
    "    # Connection to MongoDB using mongo string\n",
    "    client = MongoClient('mongodb+srv://samhithamuvva:Salmonskinroll%406@cluster0.02pxevw.mongodb.net/')\n",
    "    \n",
    "    # Select database and collection\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Insert data into MongoDB in batches\n",
    "    for i in range(0, len(parsed_data), batch_size):\n",
    "        batch_data = parsed_data[i:i+batch_size]\n",
    "        collection.insert_many(batch_data)\n",
    "        print(f\"Uploaded {min(batch_size, len(parsed_data)-i)} entries\")\n",
    "\n",
    "    # Close the MongoDB connection\n",
    "    client.close()\n",
    "    \n",
    "    print(\"CSV data uploaded to MongoDB successfully.\")\n",
    "\n",
    "# Connecting with the compass data\n",
    "csv_file = 'C:/Users/samhi/Documents/sjsu/Database/Project/Transformed_Archived_Data.csv'\n",
    "db_name = 'data225_project'\n",
    "collection_name = 'nyt-data'\n",
    "\n",
    "upload_csv_to_mongodb(csv_file, db_name, collection_name, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710df2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
